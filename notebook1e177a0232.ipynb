{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e232595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T18:59:22.580556Z",
     "iopub.status.busy": "2025-09-25T18:59:22.580279Z",
     "iopub.status.idle": "2025-09-25T20:19:48.282421Z",
     "shell.execute_reply": "2025-09-25T20:19:48.281596Z"
    },
    "papermill": {
     "duration": 4825.707249,
     "end_time": "2025-09-25T20:19:48.283780",
     "exception": false,
     "start_time": "2025-09-25T18:59:22.576531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 18:59:26.138999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758826766.360872      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1758826766.428998      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Loading data...\n",
      "Loaded 175621 sentence pairs.\n",
      "Preprocessing sentences...\n",
      "Tokenizing and padding sequences...\n",
      "English vocab size: 14120\n",
      "French vocab size: 24188\n",
      "Creating training and validation datasets...\n",
      "Total train samples: 140496\n",
      "Total validation samples: 35125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1758826789.677392      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Transformer model architecture...\n",
      "Setting up optimizer, loss, and metrics...\n",
      "Instantiating the Transformer model...\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1 Batch 0 Loss 10.0715 Accuracy 0.0000\n",
      "Epoch 1 Batch 200 Loss 9.5905 Accuracy 0.0874\n",
      "Epoch 1 Batch 400 Loss 8.5194 Accuracy 0.1270\n",
      "Epoch 1 Batch 600 Loss 7.5519 Accuracy 0.1535\n",
      "Epoch 1 Batch 800 Loss 6.8601 Accuracy 0.1876\n",
      "Epoch 1 Batch 1000 Loss 6.3499 Accuracy 0.2173\n",
      "Epoch 1 Batch 1200 Loss 5.9558 Accuracy 0.2428\n",
      "Epoch 1 Batch 1400 Loss 5.6401 Accuracy 0.2643\n",
      "Epoch 1 Batch 1600 Loss 5.3825 Accuracy 0.2828\n",
      "Epoch 1 Batch 1800 Loss 5.1623 Accuracy 0.2992\n",
      "Epoch 1 Batch 2000 Loss 4.9723 Accuracy 0.3137\n",
      "Epoch 1 Loss 4.8059 Accuracy 0.3273\n",
      "Time taken for 1 epoch: 298.17 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.8719 Accuracy 0.5017\n",
      "Epoch 2 Batch 200 Loss 2.9074 Accuracy 0.4935\n",
      "Epoch 2 Batch 400 Loss 2.8336 Accuracy 0.5043\n",
      "Epoch 2 Batch 600 Loss 2.7608 Accuracy 0.5155\n",
      "Epoch 2 Batch 800 Loss 2.6913 Accuracy 0.5271\n",
      "Epoch 2 Batch 1000 Loss 2.6293 Accuracy 0.5371\n",
      "Epoch 2 Batch 1200 Loss 2.5695 Accuracy 0.5468\n",
      "Epoch 2 Batch 1400 Loss 2.5167 Accuracy 0.5550\n",
      "Epoch 2 Batch 1600 Loss 2.4679 Accuracy 0.5630\n",
      "Epoch 2 Batch 1800 Loss 2.4242 Accuracy 0.5699\n",
      "Epoch 2 Batch 2000 Loss 2.3847 Accuracy 0.5761\n",
      "Epoch 2 Loss 2.3449 Accuracy 0.5824\n",
      "Time taken for 1 epoch: 235.04 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.9111 Accuracy 0.6417\n",
      "Epoch 3 Batch 200 Loss 1.8845 Accuracy 0.6559\n",
      "Epoch 3 Batch 400 Loss 1.8484 Accuracy 0.6604\n",
      "Epoch 3 Batch 600 Loss 1.8312 Accuracy 0.6635\n",
      "Epoch 3 Batch 800 Loss 1.8099 Accuracy 0.6674\n",
      "Epoch 3 Batch 1000 Loss 1.7917 Accuracy 0.6702\n",
      "Epoch 3 Batch 1200 Loss 1.7730 Accuracy 0.6731\n",
      "Epoch 3 Batch 1400 Loss 1.7582 Accuracy 0.6762\n",
      "Epoch 3 Batch 1600 Loss 1.7445 Accuracy 0.6787\n",
      "Epoch 3 Batch 1800 Loss 1.7310 Accuracy 0.6811\n",
      "Epoch 3 Batch 2000 Loss 1.7182 Accuracy 0.6830\n",
      "Epoch 3 Loss 1.7068 Accuracy 0.6850\n",
      "Time taken for 1 epoch: 233.60 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.6810 Accuracy 0.6863\n",
      "Epoch 4 Batch 200 Loss 1.5267 Accuracy 0.7151\n",
      "Epoch 4 Batch 400 Loss 1.5282 Accuracy 0.7160\n",
      "Epoch 4 Batch 600 Loss 1.5221 Accuracy 0.7179\n",
      "Epoch 4 Batch 800 Loss 1.5134 Accuracy 0.7199\n",
      "Epoch 4 Batch 1000 Loss 1.5080 Accuracy 0.7209\n",
      "Epoch 4 Batch 1200 Loss 1.5031 Accuracy 0.7220\n",
      "Epoch 4 Batch 1400 Loss 1.4962 Accuracy 0.7234\n",
      "Epoch 4 Batch 1600 Loss 1.4895 Accuracy 0.7245\n",
      "Epoch 4 Batch 1800 Loss 1.4829 Accuracy 0.7256\n",
      "Epoch 4 Batch 2000 Loss 1.4762 Accuracy 0.7266\n",
      "Epoch 4 Loss 1.4702 Accuracy 0.7279\n",
      "Time taken for 1 epoch: 235.81 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.1029 Accuracy 0.7767\n",
      "Epoch 5 Batch 200 Loss 1.3880 Accuracy 0.7430\n",
      "Epoch 5 Batch 400 Loss 1.3770 Accuracy 0.7440\n",
      "Epoch 5 Batch 600 Loss 1.3661 Accuracy 0.7455\n",
      "Epoch 5 Batch 800 Loss 1.3638 Accuracy 0.7459\n",
      "Epoch 5 Batch 1000 Loss 1.3672 Accuracy 0.7459\n",
      "Epoch 5 Batch 1200 Loss 1.3642 Accuracy 0.7470\n",
      "Epoch 5 Batch 1400 Loss 1.3591 Accuracy 0.7478\n",
      "Epoch 5 Batch 1600 Loss 1.3533 Accuracy 0.7486\n",
      "Epoch 5 Batch 1800 Loss 1.3507 Accuracy 0.7491\n",
      "Epoch 5 Batch 2000 Loss 1.3470 Accuracy 0.7500\n",
      "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
      "Epoch 5 Loss 1.3457 Accuracy 0.7506\n",
      "Time taken for 1 epoch: 237.80 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.5648 Accuracy 0.7334\n",
      "Epoch 6 Batch 200 Loss 1.2852 Accuracy 0.7601\n",
      "Epoch 6 Batch 400 Loss 1.2656 Accuracy 0.7617\n",
      "Epoch 6 Batch 600 Loss 1.2706 Accuracy 0.7616\n",
      "Epoch 6 Batch 800 Loss 1.2690 Accuracy 0.7631\n",
      "Epoch 6 Batch 1000 Loss 1.2742 Accuracy 0.7631\n",
      "Epoch 6 Batch 1200 Loss 1.2728 Accuracy 0.7640\n",
      "Epoch 6 Batch 1400 Loss 1.2663 Accuracy 0.7649\n",
      "Epoch 6 Batch 1600 Loss 1.2651 Accuracy 0.7652\n",
      "Epoch 6 Batch 1800 Loss 1.2628 Accuracy 0.7656\n",
      "Epoch 6 Batch 2000 Loss 1.2627 Accuracy 0.7662\n",
      "Epoch 6 Loss 1.2623 Accuracy 0.7664\n",
      "Time taken for 1 epoch: 237.29 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.1988 Accuracy 0.7892\n",
      "Epoch 7 Batch 200 Loss 1.1886 Accuracy 0.7763\n",
      "Epoch 7 Batch 400 Loss 1.1882 Accuracy 0.7765\n",
      "Epoch 7 Batch 600 Loss 1.2025 Accuracy 0.7751\n",
      "Epoch 7 Batch 800 Loss 1.2039 Accuracy 0.7761\n",
      "Epoch 7 Batch 1000 Loss 1.2083 Accuracy 0.7763\n",
      "Epoch 7 Batch 1200 Loss 1.2057 Accuracy 0.7768\n",
      "Epoch 7 Batch 1400 Loss 1.2023 Accuracy 0.7772\n",
      "Epoch 7 Batch 1600 Loss 1.2026 Accuracy 0.7771\n",
      "Epoch 7 Batch 1800 Loss 1.2045 Accuracy 0.7773\n",
      "Epoch 7 Batch 2000 Loss 1.2047 Accuracy 0.7776\n",
      "Epoch 7 Loss 1.2046 Accuracy 0.7778\n",
      "Time taken for 1 epoch: 236.81 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.1803 Accuracy 0.7882\n",
      "Epoch 8 Batch 200 Loss 1.1436 Accuracy 0.7841\n",
      "Epoch 8 Batch 400 Loss 1.1454 Accuracy 0.7852\n",
      "Epoch 8 Batch 600 Loss 1.1535 Accuracy 0.7854\n",
      "Epoch 8 Batch 800 Loss 1.1626 Accuracy 0.7853\n",
      "Epoch 8 Batch 1000 Loss 1.1640 Accuracy 0.7857\n",
      "Epoch 8 Batch 1200 Loss 1.1620 Accuracy 0.7859\n",
      "Epoch 8 Batch 1400 Loss 1.1570 Accuracy 0.7863\n",
      "Epoch 8 Batch 1600 Loss 1.1579 Accuracy 0.7862\n",
      "Epoch 8 Batch 1800 Loss 1.1599 Accuracy 0.7864\n",
      "Epoch 8 Batch 2000 Loss 1.1606 Accuracy 0.7867\n",
      "Epoch 8 Loss 1.1615 Accuracy 0.7867\n",
      "Time taken for 1 epoch: 237.28 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.9581 Accuracy 0.8023\n",
      "Epoch 9 Batch 200 Loss 1.0977 Accuracy 0.7933\n",
      "Epoch 9 Batch 400 Loss 1.1087 Accuracy 0.7924\n",
      "Epoch 9 Batch 600 Loss 1.1214 Accuracy 0.7928\n",
      "Epoch 9 Batch 800 Loss 1.1305 Accuracy 0.7929\n",
      "Epoch 9 Batch 1000 Loss 1.1327 Accuracy 0.7930\n",
      "Epoch 9 Batch 1200 Loss 1.1287 Accuracy 0.7931\n",
      "Epoch 9 Batch 1400 Loss 1.1244 Accuracy 0.7935\n",
      "Epoch 9 Batch 1600 Loss 1.1251 Accuracy 0.7936\n",
      "Epoch 9 Batch 1800 Loss 1.1272 Accuracy 0.7936\n",
      "Epoch 9 Batch 2000 Loss 1.1270 Accuracy 0.7939\n",
      "Epoch 9 Loss 1.1267 Accuracy 0.7940\n",
      "Time taken for 1 epoch: 236.70 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.1698 Accuracy 0.7744\n",
      "Epoch 10 Batch 200 Loss 1.0738 Accuracy 0.7987\n",
      "Epoch 10 Batch 400 Loss 1.0856 Accuracy 0.7983\n",
      "Epoch 10 Batch 600 Loss 1.0932 Accuracy 0.7987\n",
      "Epoch 10 Batch 800 Loss 1.0984 Accuracy 0.7989\n",
      "Epoch 10 Batch 1000 Loss 1.1001 Accuracy 0.7992\n",
      "Epoch 10 Batch 1200 Loss 1.0957 Accuracy 0.7997\n",
      "Epoch 10 Batch 1400 Loss 1.0928 Accuracy 0.8001\n",
      "Epoch 10 Batch 1600 Loss 1.0949 Accuracy 0.7999\n",
      "Epoch 10 Batch 1800 Loss 1.0974 Accuracy 0.7998\n",
      "Epoch 10 Batch 2000 Loss 1.0972 Accuracy 0.8000\n",
      "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
      "Epoch 10 Loss 1.0967 Accuracy 0.8001\n",
      "Time taken for 1 epoch: 236.99 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.0265 Accuracy 0.8043\n",
      "Epoch 11 Batch 200 Loss 1.0515 Accuracy 0.8051\n",
      "Epoch 11 Batch 400 Loss 1.0599 Accuracy 0.8053\n",
      "Epoch 11 Batch 600 Loss 1.0713 Accuracy 0.8042\n",
      "Epoch 11 Batch 800 Loss 1.0787 Accuracy 0.8045\n",
      "Epoch 11 Batch 1000 Loss 1.0744 Accuracy 0.8050\n",
      "Epoch 11 Batch 1200 Loss 1.0703 Accuracy 0.8052\n",
      "Epoch 11 Batch 1400 Loss 1.0678 Accuracy 0.8054\n",
      "Epoch 11 Batch 1600 Loss 1.0697 Accuracy 0.8056\n",
      "Epoch 11 Batch 1800 Loss 1.0719 Accuracy 0.8055\n",
      "Epoch 11 Batch 2000 Loss 1.0727 Accuracy 0.8055\n",
      "Epoch 11 Loss 1.0736 Accuracy 0.8053\n",
      "Time taken for 1 epoch: 236.96 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.0410 Accuracy 0.8177\n",
      "Epoch 12 Batch 200 Loss 1.0277 Accuracy 0.8101\n",
      "Epoch 12 Batch 400 Loss 1.0392 Accuracy 0.8100\n",
      "Epoch 12 Batch 600 Loss 1.0501 Accuracy 0.8095\n",
      "Epoch 12 Batch 800 Loss 1.0551 Accuracy 0.8092\n",
      "Epoch 12 Batch 1000 Loss 1.0549 Accuracy 0.8091\n",
      "Epoch 12 Batch 1200 Loss 1.0515 Accuracy 0.8091\n",
      "Epoch 12 Batch 1400 Loss 1.0499 Accuracy 0.8095\n",
      "Epoch 12 Batch 1600 Loss 1.0490 Accuracy 0.8098\n",
      "Epoch 12 Batch 1800 Loss 1.0527 Accuracy 0.8095\n",
      "Epoch 12 Batch 2000 Loss 1.0505 Accuracy 0.8099\n",
      "Epoch 12 Loss 1.0518 Accuracy 0.8097\n",
      "Time taken for 1 epoch: 237.16 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.0962 Accuracy 0.8229\n",
      "Epoch 13 Batch 200 Loss 1.0097 Accuracy 0.8138\n",
      "Epoch 13 Batch 400 Loss 1.0200 Accuracy 0.8147\n",
      "Epoch 13 Batch 600 Loss 1.0297 Accuracy 0.8146\n",
      "Epoch 13 Batch 800 Loss 1.0372 Accuracy 0.8138\n",
      "Epoch 13 Batch 1000 Loss 1.0364 Accuracy 0.8137\n",
      "Epoch 13 Batch 1200 Loss 1.0319 Accuracy 0.8135\n",
      "Epoch 13 Batch 1400 Loss 1.0314 Accuracy 0.8137\n",
      "Epoch 13 Batch 1600 Loss 1.0323 Accuracy 0.8137\n",
      "Epoch 13 Batch 1800 Loss 1.0331 Accuracy 0.8139\n",
      "Epoch 13 Batch 2000 Loss 1.0340 Accuracy 0.8139\n",
      "Epoch 13 Loss 1.0331 Accuracy 0.8139\n",
      "Time taken for 1 epoch: 236.81 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.9907 Accuracy 0.8291\n",
      "Epoch 14 Batch 200 Loss 1.0034 Accuracy 0.8175\n",
      "Epoch 14 Batch 400 Loss 1.0020 Accuracy 0.8185\n",
      "Epoch 14 Batch 600 Loss 1.0145 Accuracy 0.8177\n",
      "Epoch 14 Batch 800 Loss 1.0199 Accuracy 0.8172\n",
      "Epoch 14 Batch 1000 Loss 1.0181 Accuracy 0.8174\n",
      "Epoch 14 Batch 1200 Loss 1.0154 Accuracy 0.8171\n",
      "Epoch 14 Batch 1400 Loss 1.0127 Accuracy 0.8175\n",
      "Epoch 14 Batch 1600 Loss 1.0142 Accuracy 0.8174\n",
      "Epoch 14 Batch 1800 Loss 1.0161 Accuracy 0.8174\n",
      "Epoch 14 Batch 2000 Loss 1.0161 Accuracy 0.8173\n",
      "Epoch 14 Loss 1.0156 Accuracy 0.8174\n",
      "Time taken for 1 epoch: 236.55 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.0076 Accuracy 0.8169\n",
      "Epoch 15 Batch 200 Loss 0.9783 Accuracy 0.8203\n",
      "Epoch 15 Batch 400 Loss 0.9883 Accuracy 0.8199\n",
      "Epoch 15 Batch 600 Loss 0.9947 Accuracy 0.8209\n",
      "Epoch 15 Batch 800 Loss 1.0028 Accuracy 0.8205\n",
      "Epoch 15 Batch 1000 Loss 1.0018 Accuracy 0.8205\n",
      "Epoch 15 Batch 1200 Loss 0.9977 Accuracy 0.8204\n",
      "Epoch 15 Batch 1400 Loss 0.9980 Accuracy 0.8206\n",
      "Epoch 15 Batch 1600 Loss 0.9997 Accuracy 0.8205\n",
      "Epoch 15 Batch 1800 Loss 1.0014 Accuracy 0.8206\n",
      "Epoch 15 Batch 2000 Loss 1.0014 Accuracy 0.8207\n",
      "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
      "Epoch 15 Loss 0.9998 Accuracy 0.8206\n",
      "Time taken for 1 epoch: 238.29 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.8395 Accuracy 0.8404\n",
      "Epoch 16 Batch 200 Loss 0.9662 Accuracy 0.8246\n",
      "Epoch 16 Batch 400 Loss 0.9736 Accuracy 0.8242\n",
      "Epoch 16 Batch 600 Loss 0.9837 Accuracy 0.8239\n",
      "Epoch 16 Batch 800 Loss 0.9849 Accuracy 0.8237\n",
      "Epoch 16 Batch 1000 Loss 0.9853 Accuracy 0.8237\n",
      "Epoch 16 Batch 1200 Loss 0.9832 Accuracy 0.8236\n",
      "Epoch 16 Batch 1400 Loss 0.9836 Accuracy 0.8235\n",
      "Epoch 16 Batch 1600 Loss 0.9841 Accuracy 0.8235\n",
      "Epoch 16 Batch 1800 Loss 0.9860 Accuracy 0.8234\n",
      "Epoch 16 Batch 2000 Loss 0.9868 Accuracy 0.8233\n",
      "Epoch 16 Loss 0.9857 Accuracy 0.8233\n",
      "Time taken for 1 epoch: 237.25 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.7853 Accuracy 0.8157\n",
      "Epoch 17 Batch 200 Loss 0.9433 Accuracy 0.8282\n",
      "Epoch 17 Batch 400 Loss 0.9586 Accuracy 0.8274\n",
      "Epoch 17 Batch 600 Loss 0.9713 Accuracy 0.8267\n",
      "Epoch 17 Batch 800 Loss 0.9758 Accuracy 0.8259\n",
      "Epoch 17 Batch 1000 Loss 0.9713 Accuracy 0.8263\n",
      "Epoch 17 Batch 1200 Loss 0.9694 Accuracy 0.8264\n",
      "Epoch 17 Batch 1400 Loss 0.9691 Accuracy 0.8265\n",
      "Epoch 17 Batch 1600 Loss 0.9708 Accuracy 0.8265\n",
      "Epoch 17 Batch 1800 Loss 0.9723 Accuracy 0.8263\n",
      "Epoch 17 Batch 2000 Loss 0.9724 Accuracy 0.8264\n",
      "Epoch 17 Loss 0.9719 Accuracy 0.8263\n",
      "Time taken for 1 epoch: 236.92 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.8495 Accuracy 0.8414\n",
      "Epoch 18 Batch 200 Loss 0.9236 Accuracy 0.8316\n",
      "Epoch 18 Batch 400 Loss 0.9451 Accuracy 0.8309\n",
      "Epoch 18 Batch 600 Loss 0.9543 Accuracy 0.8300\n",
      "Epoch 18 Batch 800 Loss 0.9616 Accuracy 0.8295\n",
      "Epoch 18 Batch 1000 Loss 0.9613 Accuracy 0.8294\n",
      "Epoch 18 Batch 1200 Loss 0.9588 Accuracy 0.8294\n",
      "Epoch 18 Batch 1400 Loss 0.9589 Accuracy 0.8293\n",
      "Epoch 18 Batch 1600 Loss 0.9625 Accuracy 0.8291\n",
      "Epoch 18 Batch 1800 Loss 0.9635 Accuracy 0.8290\n",
      "Epoch 18 Batch 2000 Loss 0.9627 Accuracy 0.8290\n",
      "Epoch 18 Loss 0.9611 Accuracy 0.8289\n",
      "Time taken for 1 epoch: 237.63 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.8968 Accuracy 0.8410\n",
      "Epoch 19 Batch 200 Loss 0.9294 Accuracy 0.8318\n",
      "Epoch 19 Batch 400 Loss 0.9371 Accuracy 0.8318\n",
      "Epoch 19 Batch 600 Loss 0.9475 Accuracy 0.8313\n",
      "Epoch 19 Batch 800 Loss 0.9521 Accuracy 0.8311\n",
      "Epoch 19 Batch 1000 Loss 0.9479 Accuracy 0.8311\n",
      "Epoch 19 Batch 1200 Loss 0.9471 Accuracy 0.8313\n",
      "Epoch 19 Batch 1400 Loss 0.9455 Accuracy 0.8314\n",
      "Epoch 19 Batch 1600 Loss 0.9487 Accuracy 0.8313\n",
      "Epoch 19 Batch 1800 Loss 0.9506 Accuracy 0.8314\n",
      "Epoch 19 Batch 2000 Loss 0.9499 Accuracy 0.8314\n",
      "Epoch 19 Loss 0.9494 Accuracy 0.8313\n",
      "Time taken for 1 epoch: 237.43 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.0583 Accuracy 0.8076\n",
      "Epoch 20 Batch 200 Loss 0.9237 Accuracy 0.8323\n",
      "Epoch 20 Batch 400 Loss 0.9300 Accuracy 0.8340\n",
      "Epoch 20 Batch 600 Loss 0.9384 Accuracy 0.8338\n",
      "Epoch 20 Batch 800 Loss 0.9375 Accuracy 0.8337\n",
      "Epoch 20 Batch 1000 Loss 0.9387 Accuracy 0.8336\n",
      "Epoch 20 Batch 1200 Loss 0.9368 Accuracy 0.8336\n",
      "Epoch 20 Batch 1400 Loss 0.9367 Accuracy 0.8335\n",
      "Epoch 20 Batch 1600 Loss 0.9386 Accuracy 0.8333\n",
      "Epoch 20 Batch 1800 Loss 0.9403 Accuracy 0.8332\n",
      "Epoch 20 Batch 2000 Loss 0.9393 Accuracy 0.8332\n",
      "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
      "Epoch 20 Loss 0.9388 Accuracy 0.8332\n",
      "Time taken for 1 epoch: 237.88 secs\n",
      "\n",
      "--- Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 0. SETUP AND IMPORTS\n",
    "# ==============================================================================\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION AND HYPERPARAMETERS\n",
    "# ==============================================================================\n",
    "# --- Data Configuration ---\n",
    "CSV_PATH = \"/kaggle/input/language-translation-englishfrench/eng_-french.csv\"\n",
    "EN_COL = \"English words/sentences\"\n",
    "FR_COL = \"French words/sentences\"\n",
    "NUM_SAMPLES = None # Set to an integer for testing, or None for full dataset\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "MAX_LENGTH = 40\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "EPOCHS = 20\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# ==============================================================================\n",
    "# !! CRITICAL FIX !!\n",
    "# Clean up any old checkpoint files before we begin, to prevent state conflicts.\n",
    "# ==============================================================================\n",
    "if os.path.exists('./checkpoints'):\n",
    "    shutil.rmtree('./checkpoints')\n",
    "    print(\"Old checkpoints directory has been removed.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATA LOADING AND PREPARATION\n",
    "# ==============================================================================\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV file not found at {CSV_PATH}. Please update the CSV_PATH variable.\")\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "if NUM_SAMPLES:\n",
    "    df = df.head(NUM_SAMPLES)\n",
    "\n",
    "df = df[[EN_COL, FR_COL]].dropna().reset_index(drop=True)\n",
    "print(f\"Loaded {len(df)} sentence pairs.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. TEXT PREPROCESSING\n",
    "# ==============================================================================\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = str(sentence).lower()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-z0-9à-ž?.!,¿]+\", \" \", sentence, flags=re.IGNORECASE)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = \"sos \" + sentence + \" eos\"\n",
    "    return sentence\n",
    "\n",
    "print(\"Preprocessing sentences...\")\n",
    "df[\"eng_proc\"] = df[EN_COL].astype(str).apply(preprocess_sentence)\n",
    "df[\"fre_proc\"] = df[FR_COL].astype(str).apply(preprocess_sentence)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. TOKENIZATION AND PADDING\n",
    "# ==============================================================================\n",
    "print(\"Tokenizing and padding sequences...\")\n",
    "eng_tokenizer = Tokenizer(filters='', oov_token='<unk>')\n",
    "eng_tokenizer.fit_on_texts(df[\"eng_proc\"].tolist())\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(df[\"eng_proc\"].tolist())\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "fre_tokenizer = Tokenizer(filters='', oov_token='<unk>')\n",
    "fre_tokenizer.fit_on_texts(df[\"fre_proc\"].tolist())\n",
    "fre_sequences = fre_tokenizer.texts_to_sequences(df[\"fre_proc\"].tolist())\n",
    "fre_vocab_size = len(fre_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"English vocab size:\", eng_vocab_size)\n",
    "print(\"French vocab size:\", fre_vocab_size)\n",
    "\n",
    "eng_padded = pad_sequences(eng_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "fre_padded = pad_sequences(fre_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. CREATE TF.DATA DATASETS\n",
    "# ==============================================================================\n",
    "print(\"Creating training and validation datasets...\")\n",
    "eng_train, eng_val, fre_train, fre_val = train_test_split(eng_padded, fre_padded, test_size=0.2, random_state=42)\n",
    "print(\"Total train samples:\", len(eng_train))\n",
    "print(\"Total validation samples:\", len(eng_val))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((eng_train, fre_train))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((eng_val, fre_val)).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. TRANSFORMER MODEL DEFINITION\n",
    "# ==============================================================================\n",
    "print(\"Defining Transformer model architecture...\")\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = np.arange(position)[:, np.newaxis] / np.power(10000, (2 * (np.arange(d_model)[np.newaxis, :] // 2)) / np.float32(d_model))\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def create_masks(inp, tar_inp):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar_inp)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar_inp)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = d_model // num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q, k, v = self.wq(q), self.wk(k), self.wv(v)\n",
    "        q = self.split_heads(q, batch_size); k = self.split_heads(k, batch_size); v = self.split_heads(v, batch_size)\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.d_model))\n",
    "        out = self.dense(concat_attention)\n",
    "        return out, attention_weights\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(d_model)])\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, *, training, mask):\n",
    "        attn_out, _ = self.mha(x, x, x, mask)\n",
    "        attn_out = self.dropout1(attn_out, training=training)\n",
    "        out1 = self.layernorm1(x + attn_out)\n",
    "        ffn_out = self.ffn(out1)\n",
    "        ffn_out = self.dropout2(ffn_out, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_out)\n",
    "        return out2\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, *, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_w1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        attn2, attn_w2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        ffn_out = self.ffn(out2)\n",
    "        ffn_out = self.dropout3(ffn_out, training=training)\n",
    "        out3 = self.layernorm3(ffn_out + out2)\n",
    "        return out3, attn_w1, attn_w2\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, max_seq_len, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers  # <--- FIX WAS HERE\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, *, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, max_seq_len, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model      # <--- FIX WAS HERE\n",
    "        self.num_layers = num_layers  # <--- FIX WAS HERE\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, *, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(x, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output=enc_output, training=training, look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "        return x, attention_weights\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        inp, tar_inp = inputs\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(x=tar_inp, enc_output=enc_output, training=training, look_ahead_mask=combined_mask, padding_mask=dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output, attention_weights\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. OPTIMIZER, LOSS, AND METRICS\n",
    "# ==============================================================================\n",
    "print(\"Setting up optimizer, loss, and metrics...\")\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9, clipnorm=1.0)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\n",
    "\n",
    "def masked_accuracy(real, pred):\n",
    "    pred_ids = tf.argmax(pred, axis=-1, output_type=real.dtype)\n",
    "    matches = tf.cast(tf.equal(real, pred_ids), tf.float32)\n",
    "    mask = tf.cast(tf.not_equal(real, 0), tf.float32)\n",
    "    matches *= mask\n",
    "    return tf.reduce_sum(matches) / tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
    "\n",
    "# ==============================================================================\n",
    "# 8. INSTANTIATE MODEL AND SET UP CHECKPOINTS\n",
    "# ==============================================================================\n",
    "print(\"Instantiating the Transformer model...\")\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "    input_vocab_size=eng_vocab_size, target_vocab_size=fre_vocab_size,\n",
    "    pe_input=MAX_LENGTH, pe_target=MAX_LENGTH, rate=dropout_rate\n",
    ")\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Latest checkpoint restored!')\n",
    "\n",
    "# ==============================================================================\n",
    "# 9. TRAINING\n",
    "# ==============================================================================\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer((inp, tar_inp), training=True)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(masked_accuracy(tar_real, predictions))\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    train_loss.reset_state()\n",
    "    train_accuracy.reset_state()\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 200 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n",
    "\n",
    "print(\"--- Training Finished ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec004726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T20:19:48.305408Z",
     "iopub.status.busy": "2025-09-25T20:19:48.304941Z",
     "iopub.status.idle": "2025-09-25T20:19:52.441608Z",
     "shell.execute_reply": "2025-09-25T20:19:52.440924Z"
    },
    "papermill": {
     "duration": 4.148636,
     "end_time": "2025-09-25T20:19:52.442845",
     "exception": false,
     "start_time": "2025-09-25T20:19:48.294209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Translations ---\n",
      "Input: hello world\n",
      "Predicted translation: bonjour .\n",
      "Input: this is a test\n",
      "Predicted translation: c est un contrôle .\n",
      "Input: I love to learn new things.\n",
      "Predicted translation: j adore apprendre de nouvelles choses .\n",
      "Input: She is a great writer.\n",
      "Predicted translation: elle est un bon écrivain .\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 10. EVALUATION AND TRANSLATION (CORRECTED)\n",
    "# ==============================================================================\n",
    "\n",
    "# Define the corrected evaluate function\n",
    "def evaluate(inp_sentence):\n",
    "    inp_sentence_proc = preprocess_sentence(inp_sentence)\n",
    "    inp_tensor = tf.convert_to_tensor([eng_tokenizer.texts_to_sequences([inp_sentence_proc])[0]])\n",
    "    inp_tensor = pad_sequences(inp_tensor, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "    decoder_input = [fre_tokenizer.word_index['sos']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions, _ = transformer((inp_tensor, output), training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        \n",
    "        # **** THIS IS THE FIX ****\n",
    "        # We specify the output_type to match the 'output' tensor's type (int32).\n",
    "        predicted_id = tf.argmax(predictions, axis=-1, output_type=tf.int32)\n",
    "        \n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "        # Also, ensure we're comparing an int to an int\n",
    "        if predicted_id[0][0] == fre_tokenizer.word_index['eos']:\n",
    "            break\n",
    "            \n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "# Define the corrected translate function\n",
    "def translate(sentence):\n",
    "    result = evaluate(sentence)\n",
    "    predicted_sentence = fre_tokenizer.sequences_to_texts([result.numpy()])\n",
    "    predicted_sentence = predicted_sentence[0].replace('sos ', '').replace(' eos', '').strip()\n",
    "    print(f'Input: {sentence}')\n",
    "    print(f'Predicted translation: {predicted_sentence}')\n",
    "\n",
    "# --- Test your model ---\n",
    "print(\"\\n--- Testing Translations ---\")\n",
    "translate(\"hello world\")\n",
    "translate(\"this is a test\")\n",
    "translate(\"I love to learn new things.\")\n",
    "translate(\"She is a great writer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea787c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-25T20:19:52.464545Z",
     "iopub.status.busy": "2025-09-25T20:19:52.463928Z",
     "iopub.status.idle": "2025-09-25T20:19:52.879472Z",
     "shell.execute_reply": "2025-09-25T20:19:52.878629Z"
    },
    "papermill": {
     "duration": 0.427582,
     "end_time": "2025-09-25T20:19:52.880804",
     "exception": false,
     "start_time": "2025-09-25T20:19:52.453222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the Transformer model...\n",
      "Model saved successfully to: /kaggle/working/my_translation_model_assets/transformer_model.keras\n",
      "Saving the tokenizers...\n",
      "Tokenizers saved successfully to: /kaggle/working/my_translation_model_assets/eng_tokenizer.pkl and /kaggle/working/my_translation_model_assets/fre_tokenizer.pkl\n",
      "\n",
      "IMPORTANT: Your model is now saved. To make it permanent, click the 'Save Version' button and choose 'Save & Run All (Commit)'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Define the output directory in the Kaggle environment\n",
    "SAVE_DIR = '/kaggle/working/my_translation_model_assets'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# 1. Save the entire Transformer model with the .keras extension\n",
    "print(\"Saving the Transformer model...\")\n",
    "\n",
    "# **** THIS IS THE FIX ****\n",
    "# We add the '.keras' extension to the filename.\n",
    "model_path = os.path.join(SAVE_DIR, 'transformer_model.keras')\n",
    "transformer.save(model_path)\n",
    "print(f\"Model saved successfully to: {model_path}\")\n",
    "\n",
    "\n",
    "# 2. Save the tokenizers (this part was already correct)\n",
    "print(\"Saving the tokenizers...\")\n",
    "tokenizer_eng_path = os.path.join(SAVE_DIR, 'eng_tokenizer.pkl')\n",
    "with open(tokenizer_eng_path, 'wb') as f:\n",
    "    pickle.dump(eng_tokenizer, f)\n",
    "\n",
    "tokenizer_fre_path = os.path.join(SAVE_DIR, 'fre_tokenizer.pkl')\n",
    "with open(tokenizer_fre_path, 'wb') as f:\n",
    "    pickle.dump(fre_tokenizer, f)\n",
    "\n",
    "print(f\"Tokenizers saved successfully to: {tokenizer_eng_path} and {tokenizer_fre_path}\")\n",
    "print(\"\\nIMPORTANT: Your model is now saved. To make it permanent, click the 'Save Version' button and choose 'Save & Run All (Commit)'.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 592212,
     "sourceId": 1067156,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4838.287141,
   "end_time": "2025-09-25T20:19:56.313274",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-25T18:59:18.026133",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
